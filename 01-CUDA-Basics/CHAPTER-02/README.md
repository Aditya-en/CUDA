# Chapter 2: Heterogeneous Data Parallel Computing

## Key Concepts

- **Data parallelism**: Involves distributing data across multiple parallel computing resources. Each processing unit (thread) performs the same operation on different parts of the dataset. This allows computational tasks to be completed simultaneously, improving performance. It's a core concept in CUDA, where GPUs excel at parallel tasks.
- **CUDA C**: A parallel computing platform and programming model created by NVIDIA, which extends standard ANSI C with new syntax and library functions. This extension enables writing programs that execute on both CPUs (hosts) and GPUs (devices). CUDA abstracts the GPU's complexity, allowing developers to utilize GPUs for general-purpose computing.

- **Host (CPU) and Device (GPU) code coexistence**: In a CUDA C program, both CPU and GPU code coexist. The CPU is responsible for setting up and managing the execution of parallel code on the GPU. Communication between host and device occurs through specific API functions that handle memory allocation and data transfers.

## CUDA C Program Structure

- **Execution begins on the host (CPU)**: All CUDA programs start with standard serial code executed on the CPU. The host is responsible for preparing data, launching parallel computation, and gathering results.

- **Kernel functions**: Special functions that are executed on the GPU. These functions define the computations performed by the threads in parallel. The kernel is invoked from the host but executed on the device, and each invocation launches a grid of threads on the GPU.

- **Grids and thread blocks**:

  - Grids are collections of thread blocks. Each grid is a 1D, 2D, or 3D structure that organizes the work done by the threads.
  - A **thread block** is a collection of threads that can cooperate with each other via shared memory and can synchronize execution within the block. The number of threads per block and the number of blocks in a grid are crucial for performance tuning.
  - Threads in a block can efficiently share data and coordinate their execution, but threads in different blocks cannot communicate directly.

- **Single Program Multiple Data (SPMD)**: CUDA uses the SPMD paradigm, meaning that each thread executes the same program (kernel function), but on different pieces of data. The threads are uniquely identified by their built-in IDs, allowing them to operate on different subsets of the data.

## Device Global Memory and Data Transfer

- **Global memory on the device (GPU)**: The GPU has its own large but relatively slow global memory, where the data for computations resides. Itâ€™s accessible by all threads but has higher latency compared to other types of memory (e.g., shared memory or registers).

- **Host-device memory transfer**: Before a GPU kernel can be executed, data must be transferred from the host memory to the device memory. This transfer is handled by specific runtime API functions:

  - `cudaMalloc`: Allocates memory on the GPU (device).
  - `cudaFree`: Frees the allocated memory on the GPU after computation is complete to avoid memory leaks.
  - `cudaMemcpy`: Copies data between host and device. This function specifies the direction of the copy (host-to-device or device-to-host), which can be a bottleneck if data transfer is frequent.

- **Latency of data transfer**: Transferring data between host and device can introduce significant overhead, and minimizing these transfers is important for achieving good performance in CUDA applications.

## Kernel Functions and Threading

- **Kernel functions**: These define the operations to be performed by the threads in parallel. The same function is executed by every thread in the grid, but each thread can operate on different data based on its unique index.

- **Thread hierarchy**: Threads are organized in a hierarchy that facilitates scalability:
  - **Grids**: The top-level structure, which contains multiple thread blocks.
  - **Thread blocks**: A collection of threads that work together, usually on related data. Threads within a block can cooperate and synchronize using shared memory.
- **Built-in variables**:

  - `threadIdx`: Identifies a thread within its block.
  - `blockIdx`: Identifies a block within the grid.
  - `blockDim`: Gives the dimensions of the thread block (the number of threads in each block).

  These variables allow each thread to calculate its unique index in the overall grid, which is essential for partitioning data across the threads.

## Function Qualifiers

CUDA C introduces three key function qualifiers to distinguish where functions are executed and where they are called from:

- `__global__`: Declares a kernel function that is called from the host (CPU) but executed on the device (GPU).
- `__device__`: Declares a function that is called from within a kernel or another device function and is executed on the GPU.
- `__host__`: Declares a standard function that is called from the host and executed on the host (CPU).

Functions can be qualified with both `__host__` and `__device__`, allowing them to be executed on both the CPU and GPU.

## Kernel Execution

- **Kernel launch**: A kernel is launched using special syntax: `<<<gridDim, blockDim>>>`, where:

  - `gridDim` specifies the number of blocks in the grid.
  - `blockDim` specifies the number of threads in each block.

  This configuration determines how many threads are launched and how they are organized. CUDA allows flexible configurations with 1D, 2D, or 3D grids and blocks, enabling efficient mapping to different problem dimensions.

## Vector Addition Example

- A simple CUDA program often used to demonstrate basic concepts is vector addition:

  - The host (CPU) allocates memory, transfers data to the device, and launches a kernel to perform element-wise addition of two vectors on the GPU.
  - The kernel runs on the GPU, with each thread responsible for adding one element from the vectors.
  - After computation, the results are transferred back to the host.

  This example highlights basic CUDA program structure and illustrates memory management, data transfer, and parallel execution on the GPU.

## Compilation Process

- **NVCC (NVIDIA C Compiler)**: NVCC is responsible for compiling CUDA programs. It separates host code from device code:

  - Host code is compiled using the standard C/C++ compiler (e.g., GCC).
  - Device code is compiled into PTX (Parallel Thread Execution), which is an intermediate representation for NVIDIA GPUs. PTX is then converted into machine code (SASS) for execution on the GPU.

  NVCC automatically manages this process, producing an executable that can run on systems with CUDA-capable GPUs.

## Best Practices

- **Error checking**: Always check the return status of CUDA API calls (e.g., `cudaMalloc`, `cudaMemcpy`) to detect failures, especially in memory allocation or kernel launches.

- **Computation vs. data transfer balance**: Ensure that the computational workload on the GPU justifies the cost of data transfer between host and device. For performance-critical applications, the time spent on data transfer should be minimized relative to the time spent on computation.

- **Scalability**: When designing CUDA kernels, consider how the program will scale across different GPUs with varying numbers of cores and memory capacities.

## Limitations of the Example

- **Overhead in simple vector addition**: The example of vector addition, while simple, may not efficiently utilize the GPU due to the relatively small amount of computation compared to the overhead of data transfer between the host and device.

- **Real-world applications**: In practical applications, data is often kept in the device memory across multiple kernel invocations to reduce transfer overhead. Additionally, real-world problems typically involve more complex computations, making the GPU more advantageous.

## Summary of CUDA C Extensions

- **Function declaration keywords**:
  - `__global__`: Kernel functions (executed on device, called from host).
  - `__device__`: Device functions (executed on device, called from device).
  - `__host__`: Host functions (executed on host, called from host).
- **Kernel call syntax**: Uses the `<<<gridDim, blockDim>>>` execution configuration to launch parallel threads.
- **Built-in variables**: `threadIdx`, `blockIdx`, and `blockDim` allow threads to calculate their unique indices in the grid.

- **Runtime API functions**: Key functions for memory management (`cudaMalloc`, `cudaFree`) and data transfer (`cudaMemcpy`) between host and device.
